BachNormalisation --> did not work by itself.

ANN Layer behind Flattening --> 1.0: It may have worked added together with Dropout. Disable dropout and try again.
                                  1.1: Dropout removed, overlearning not prevented. accuracy: 0.9777 ,val_accuracy: 0.6803 better than without, but there is overlearning.

Dropout --> rate= 0.30: overlearning is prevented but it trains very slowly. Epoch=25 val_accuracy: 0.6220 unbalanced. The number of epochs should be increased.
            rate= 0.25: overlearning partially prevented Epoch count is sufficient. Epoch=25 accuracy: 0.9178 val_accuracy: 0.7406 balanced but insufficient prediction accuracy. *Version 0.5
            rate= 0.20: overlearning prevented Epoch=50 accuracy: 0.7745 val_accuracy: 0.7385 balanced prediction accuracy insufficient.
            rate= 0.30: overlearning prevented Epoch=100 accuracy: 0.7194 val_accuracy: 0.7218 balanced prediction accuracy insufficient.

Fourth Layer --> Epoch=25 rate=0.25 fourth layer added overlearning prevented test score stable. accuracy: 0.8331 val_accuracy: 0.7508 prediction accuracy insufficient. *Version 0.6

Learning Rate Scheduler --> Epoch=25 Dropout_rate=0.25 scheduler: Epoch<10 reduction -0.1 accuracy: 0.8669 val_accuracy: 0.7556 prediction accuracy insufficient.
                            Reduced the dropout value, interfered with the learning rate earlier and the result was negative.
                            Dropout removed over fitting could not be prevented.
                            Increase epoch. 

TensorflowGPU --> Performance increased, training time decreased, test accuracy increased. accuracy: 0.8722 val_accuracy: 0.7642 *Version 0.8

Grayscale --> Converted data set from 'RGB' to 'Grayscale'. accuracy: 0.8999 val_accuracy: 0.7693 *Version 1.1
 
--> The number of neural cells has been halved in total. Overlearning prevented, accuracy is low.
--> Learning Rate Scheduler switched off. Number of neural cells halved in total. Epoch=50 fixed after 30. Accuracy low.
 
